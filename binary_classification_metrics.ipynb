{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec7eb19-528e-4f6a-96f6-fde90ca0e836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda create -n metrics python=3.11 ipykernel -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e2f826-1c7e-4f7a-93d8-d990947e6963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!activate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b86ad4e-6ba8-4a0f-8c0e-0418ea03a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m ipykernel install --user --name=metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddaf5cd0-cca9-4546-9d69-0d94da88e451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /Users/kallol/miniconda3\n",
      "metrics               *  /Users/kallol/miniconda3/envs/metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2272d4-5e90-4cc0-8a1d-3031b05250f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kallol/miniconda3/envs/metrics/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842c72c-866d-4779-b5ff-3af834e78e69",
   "metadata": {},
   "source": [
    "#### scenario 1: when dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c35acc47-a10f-4c01-92f1-634b12878480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random two arrays and considering them as actuals and predicted from any ML model\n",
    "import random\n",
    "\n",
    "actuals = [random.randrange(0,1) for i in range(0,1000)]+[0,0,0,0,1]   #making the data imbalanced\n",
    "predicted=[random.randrange(0,1) for i in range(0,1000)]+[1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac0846d9-ea10-45a5-89eb-b87d7f711942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1004\n",
      "           1       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           1.00      1005\n",
      "   macro avg       0.60      1.00      0.67      1005\n",
      "weighted avg       1.00      1.00      1.00      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actuals,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "54ab09b2-7f2b-480e-bd9b-b30e6c4e8191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    4]\n",
      " [   0    1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(actuals,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f75b088-5b4d-4a8a-b473-e05839356636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 0, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(actuals,predicted).ravel()\n",
    "tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3dd4b136-7c4a-4d2e-b1c4-a130d36fe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# calculating true positive rate - which is recall - basically saying that - out of total positive classes we are predicting - how many of them is actually positive \n",
    "\n",
    "tpr=tp/(tp+fn)\n",
    "print(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0065c4d2-3058-49f7-8803-3637805cc4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00398406374501992\n"
     ]
    }
   ],
   "source": [
    "fpr=fp/(fp+tn)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "036b4016-a7f1-41ed-b577-136d55efca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99800796812749"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auc-roc score when imbalance dataset is there \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(actuals,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0ffd2-d9d0-4c73-9821-087899a49332",
   "metadata": {},
   "source": [
    "###### above roc-auc is very high because it considers two things- true positive rate and false positive rate, here true positive rate is very high and false positive is very low due less no of instances - eventhough the ROC is very high the model is not so great the below blog contains when to apply which metrics in binary classification scenario \n",
    "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd3818-bcef-466d-bf8e-08cf6dc0606f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9877ee99-ddec-47d9-9c4b-d0e92475e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this kind of scenarios, we should be using ppv- positive predicive value(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ece0f078-fbe0-4f27-ae9c-280e27f4ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# out of all class predicted as positive how many of them actually positive\n",
    "ppv=tp/(tp+fp)\n",
    "print(ppv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373362f-13dd-41b6-a5db-317bf302d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0d014-9dc6-403b-bee0-87357bab1bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a51dae-4bf1-4baf-84da-417913f5a84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c3e952c-15c3-49f2-af89-be4b35893221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "086a76fa-1674-4033-9506-4ae10744b6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.95024876e-04, 2.00000000e-01, 1.00000000e+00]),\n",
       " array([1., 1., 0.]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(actuals, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37981471-3581-4ca3-87b1-7df1c74b8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39a36045-396d-4ed5-a404-885e110c7a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4176b-bedd-468f-b106-ca3fdfa08c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a970e81-4d82-42b0-a5e6-a7f356314489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "metrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
